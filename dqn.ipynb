{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'YOLOv8' from 'yolov8' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/yolov8/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageFont, ImageDraw, Image\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mssl\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39myolov8\u001b[39;00m \u001b[39mimport\u001b[39;00m YOLOv8\n\u001b[1;32m     10\u001b[0m \u001b[39m# Initialize YOLOv8 model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m YOLOv8(\u001b[39m\"\u001b[39m\u001b[39myolov8n.pt\u001b[39m\u001b[39m\"\u001b[39m, classes_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdnn_model/classes.txt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'YOLOv8' from 'yolov8' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/yolov8/__init__.py)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import ssl\n",
    "from yolov8 import YOLOv8\n",
    "\n",
    "# Initialize YOLOv8 model\n",
    "model = YOLOv8(\"yolov8n.pt\", classes_path=\"dnn_model/classes.txt\")\n",
    "\n",
    "# Use the model for object detection\n",
    "detections = model.detect(frame)\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Define the ObjectDetection class for YOLOv8 object detection\n",
    "class ObjectDetection:\n",
    "    def __init__(self, model_weights='yolov8n.pt', classes_path='dnn_model/classes.txt'):\n",
    "        print(\"Loading Object Detection\")\n",
    "        print(\"Running YOLOv8\")\n",
    "\n",
    "        self.confThreshold = 0.5\n",
    "        self.nmsThreshold = 0.4\n",
    "\n",
    "        # Load the YOLOv8 model\n",
    "        self.net = cv2.dnn.readNet(model_weights)\n",
    "        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "        self.current_object_id = None  # Initialize the current_object_id to None\n",
    "        # Load the class names\n",
    "        self.load_class_names(classes_path)\n",
    "\n",
    "    def load_class_names(self, classes_path):\n",
    "        with open(classes_path, \"r\") as file_object:\n",
    "            self.classes = [class_name.strip() for class_name in file_object]\n",
    "\n",
    "        self.colors = np.random.uniform(0, 255, size=(len(self.classes), 3))\n",
    "\n",
    "    def detect(self, frame):\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "        output_layers = self.net.getUnconnectedOutLayersNames()\n",
    "        outputs = self.net.forward(output_layers)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        for output in outputs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > self.confThreshold:\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    class_ids.append(int(class_id))\n",
    "                    confidences.append(float(confidence))\n",
    "                    boxes.append((x, y, w, h))\n",
    "\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, self.confThreshold, self.nmsThreshold)\n",
    "        filtered_indices = indices.flatten()\n",
    "        filtered_class_ids = [class_ids[i] for i in filtered_indices]\n",
    "        filtered_confidences = [confidences[i] for i in filtered_indices]\n",
    "        filtered_boxes = [boxes[i] for i in filtered_indices]\n",
    "\n",
    "        return filtered_class_ids, filtered_confidences, filtered_boxes\n",
    "\n",
    "# Define the DQN class (you can use your existing DQN class)\n",
    "\n",
    "# Define the ObjectTracker class\n",
    "class ObjectTracker:\n",
    "    def __init__(self, object_detection, dqn_model_path, video_path, output_video_path):\n",
    "        self.object_detection = object_detection\n",
    "        self.dqn_model_path = dqn_model_path\n",
    "        self.video_path = video_path\n",
    "        self.output_video_path = output_video_path\n",
    "\n",
    "        # Initialize other parameters and variables\n",
    "\n",
    "    def run(self):\n",
    "        # Load the trained DQN model\n",
    "        dqn_model = DQN(self.state_size, self.action_size)\n",
    "        dqn_model.load_state_dict(torch.load(self.dqn_model_path))\n",
    "        dqn_model.eval()\n",
    "\n",
    "        # Open the video capture\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "\n",
    "        # Initialize the output video writer\n",
    "        output_video = self.initialize_output_video_writer(cap)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Perform object detection\n",
    "            detections = self.object_detection.detect(frame)\n",
    "\n",
    "            # Update tracking objects using Kalman filtering and Hungarian matching\n",
    "            self.update_tracking_objects(detections)\n",
    "\n",
    "            # Perform action and update states\n",
    "            action = self.choose_action()\n",
    "            reward = self.calculate_reward(action)\n",
    "            next_state = self.get_next_state()\n",
    "            done = self.check_episode_completion()\n",
    "\n",
    "            # Update total reward and other counters\n",
    "\n",
    "            # Store the transition in the replay buffer\n",
    "\n",
    "            # Perform DQN training\n",
    "\n",
    "            # Update the target network periodically\n",
    "\n",
    "            # Write frame with bounding boxes and labels to output video\n",
    "            self.write_frame_to_output_video(frame, output_video)\n",
    "\n",
    "            # Show the frame with the detected objects\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        # Release video capture and close windows\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Print the summary of detected objects\n",
    "        self.print_detected_objects_summary()\n",
    "\n",
    "        # Print the tracked objects\n",
    "        self.print_tracked_objects()\n",
    "\n",
    "    # Implement the rest of the methods\n",
    "\n",
    "# Create an instance of the ObjectDetection class\n",
    "od = ObjectDetection(model_weights='yolov8n.pt', classes_path='dnn_model/classes.txt')\n",
    "\n",
    "# Specify the paths\n",
    "dqn_model_path = 'modell.h5'\n",
    "video_path = 'output.mp4'\n",
    "output_video_path = 'tracked_output.mp4'\n",
    "\n",
    "# Create an instance of the ObjectTracker class and run the object tracking\n",
    "tracker = ObjectTracker(od, dqn_model_path, video_path, output_video_path)\n",
    "tracker.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 204 (4183001720.py, line 205)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 205\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.object_id_counter += 1\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 204\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    " \n",
    "\n",
    "class ObjectDetection:\n",
    "    def __init__(self, model_config='dnn_model/yolov4.cfg', model_weights='dnn_model/yolov4.weights',\n",
    "            classes_path='dnn_model/classes.txt'):\n",
    "        print(\"Loading Object Detection\")\n",
    "        print(\"Running YOLOv8\")\n",
    "\n",
    "        self.confThreshold = 0.5\n",
    "        self.nmsThreshold = 0.4\n",
    "\n",
    "        # Load the YOLOv4 model\n",
    "        self.net = cv2.dnn.readNetFromDarknet(model_config, model_weights)\n",
    "        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "        self.current_object_id = None  # Initialize the current_object_id to None\n",
    "        # Load the class names\n",
    "        self.load_class_names(classes_path)\n",
    "\n",
    "    def load_class_names(self, classes_path):\n",
    "        with open(classes_path, \"r\") as file_object:\n",
    "            self.classes = [class_name.strip() for class_name in file_object]\n",
    "\n",
    "        self.colors = np.random.uniform(0, 255, size=(len(self.classes), 3))\n",
    "\n",
    "    def detect(self, frame):\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "        output_layers = self.net.getUnconnectedOutLayersNames()\n",
    "        outputs = self.net.forward(output_layers)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        for output in outputs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > self.confThreshold:\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    class_ids.append(int(class_id))\n",
    "                    confidences.append(float(confidence))\n",
    "                    boxes.append((x, y, w, h))\n",
    "\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, self.confThreshold, self.nmsThreshold)\n",
    "        filtered_indices = indices.flatten()\n",
    "        filtered_class_ids = [class_ids[i] for i in filtered_indices]\n",
    "        filtered_confidences = [confidences[i] for i in filtered_indices]\n",
    "        filtered_boxes = [boxes[i] for i in filtered_indices]\n",
    "\n",
    "        return filtered_class_ids, filtered_confidences, filtered_boxes\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ObjectTracker:\n",
    "    def __init__(self, object_detection, dqn_model_path, video_path, output_video_path):\n",
    "        self.object_detection = object_detection\n",
    "        self.dqn_model_path = dqn_model_path\n",
    "        self.video_path = video_path\n",
    "        self.output_video_path = output_video_path\n",
    "\n",
    "        self.state_size = 150528\n",
    "        self.action_size = 2\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_episodes = 500\n",
    "        self.num_steps = 100\n",
    "        self.target_update_freq = 10\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.epsilon_min = 0.01\n",
    "        self.gamma = 0.99\n",
    "        self.target_objects = ['car', 'person']\n",
    "\n",
    "        self.tracking_objects = {}\n",
    "        self.object_id_counter = 0\n",
    "        self.total_reward = 0\n",
    "        self.correct_target_count = 0\n",
    "        self.incorrect_target_count = 0\n",
    "        self.no_target_count = 0\n",
    "        self.movement_count = 0\n",
    "\n",
    "    def run(self):\n",
    "        # Load the trained DQN model\n",
    "        dqn_model = DQN(self.state_size, self.action_size)\n",
    "        dqn_model.load_state_dict(torch.load(self.dqn_model_path))\n",
    "        dqn_model.eval()\n",
    "\n",
    "        # Open the video capture\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "\n",
    "        # Initialize the output video writer\n",
    "        output_video = self.initialize_output_video_writer(cap)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Perform object detection\n",
    "            detections = self.object_detection.detect(frame)\n",
    "\n",
    "            # Update tracking objects\n",
    "            self.update_tracking_objects(detections)\n",
    "\n",
    "            # Perform action and update states\n",
    "            action = self.choose_action()\n",
    "            reward = self.calculate_reward(action)\n",
    "            next_state = self.get_next_state()\n",
    "            done = self.check_episode_completion()\n",
    "\n",
    "            # Update total reward and other counters\n",
    "\n",
    "            # Store the transition in the replay buffer\n",
    "\n",
    "            # Perform DQN training\n",
    "\n",
    "            # Update the target network periodically\n",
    "\n",
    "            # Write frame with bounding boxes and labels to output video\n",
    "            self.write_frame_to_output_video(frame, output_video)\n",
    "\n",
    "            # Show the frame with the detected objects\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        # Release video capture and close windows\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Print the summary of detected objects\n",
    "        self.print_detected_objects_summary()\n",
    "\n",
    "        # Print the tracked objects\n",
    "        self.print_tracked_objects()\n",
    "\n",
    "    def initialize_output_video_writer(self, cap):\n",
    "        output_video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        output_video = cv2.VideoWriter(self.output_video_path, fourcc, output_video_fps,\n",
    " (frame_width, frame_height), True)\n",
    "        return output_video\n",
    "\n",
    "    def update_tracking_objects(self, detections):\n",
    "        class_ids, confidences, boxes = detections\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            print(\"No objects detected.\")\n",
    "            return\n",
    "        max_confidence_index = np.argmax(confidences)\n",
    "        max_confidence_box = boxes[max_confidence_index]\n",
    "        class_id = class_ids[max_confidence_index]\n",
    "\n",
    "        class_name = self.object_detection.classes[class_id]\n",
    "        object_id = self.tracking_objects.get(class_name)\n",
    "        for i, box in enumerate(boxes):\n",
    "            class_id = class_ids[i]\n",
    "            class_name = self.object_detection.classes[class_id]\n",
    "            (x, y, w, h) = box\n",
    "\n",
    "            if class_name not in self.tracking_objects:\n",
    "                self.object_id_counter += 1\n",
    "                self.tracking_objects[class_name] = self.object_id_counter\n",
    "\n",
    "            if object_id is None:\n",
    "            self.object_id_counter += 1\n",
    "            self.tracking_objects[class_name] = self.object_id_counter\n",
    "            object_id = self.object_id_counter\n",
    "            self.current_object_id = object_id\n",
    "            self.current_bounding_box = max_confidence_box\n",
    "            # Update object position and other information\n",
    "\n",
    "    def choose_action(self):\n",
    "        # Implement action selection logic (e.g., epsilon-greedy policy)\n",
    "        if random.random() < self.epsilon:\n",
    "            # Random action (exploration)\n",
    "            return random.choice([0, 1])  # Example: 0 for left, 1 for right\n",
    "        else:\n",
    "            # Choose action based on DQN\n",
    "            state = self.get_current_state()  # Implement this function\n",
    "            with torch.no_grad():\n",
    "                q_values = self.dqn_model(state)\n",
    "            action = q_values.argmax().item()\n",
    "            return action\n",
    "    def calculate_reward(self, action):\n",
    "        bounding_box = self.get_current_bounding_box()\n",
    "        frame_width, frame_height = self.get_frame_dimensions()\n",
    "\n",
    "        middle_50_start = frame_width // 4\n",
    "        middle_50_end = frame_width - frame_width // 4\n",
    "        x1 = bounding_box[0]\n",
    "        x2 = bounding_box[0] + bounding_box[2]\n",
    "\n",
    "        if middle_50_start < x1 < x2 < middle_50_end:\n",
    "            # Calculate distance reward (example)\n",
    "            distance_reward = self.calculate_distance_reward(bounding_box, frame_width, frame_height)\n",
    "            return distance_reward\n",
    "        else:\n",
    "            # Apply penalty (example)\n",
    "            penalty = self.calculate_penalty(bounding_box, frame_width, frame_height)\n",
    "            return penalty\n",
    "    def calculate_distance_reward(self, bounding_box, frame_width, frame_height):\n",
    "        # Implement your logic to calculate distance-based reward\n",
    "        # Example: Calculate the distance between the bounding box center and frame center\n",
    "        box_center_x = bounding_box[0] + bounding_box[2] // 2\n",
    "        frame_center_x = frame_width // 2\n",
    "        distance = abs(box_center_x - frame_center_x)\n",
    "        max_distance = frame_width // 2  # Maximum possible distance\n",
    "        normalized_distance = 1 - (distance / max_distance)  # Normalize to [0, 1]\n",
    "        return normalized_distance\n",
    "\n",
    "    def calculate_penalty(self, bounding_box, frame_width, frame_height):\n",
    "        # Implement your logic to calculate penalties\n",
    "        # Example: Penalize for being outside the middle 50%\n",
    "        middle_50_start = frame_width // 4\n",
    "        middle_50_end = frame_width - frame_width // 4\n",
    "        x1 = bounding_box[0]\n",
    "        x2 = bounding_box[0] + bounding_box[2]\n",
    "\n",
    "        if x1 < middle_50_start or x2 > middle_50_end:\n",
    "            # Penalize for being outside the middle 50%\n",
    "            return -0.5  # Example penalty value\n",
    "        else:\n",
    "            return 0  # No penalty\n",
    "\n",
    "    def get_next_state(self):\n",
    "        # Implement your logic to get the next state\n",
    "      return\n",
    "    def check_episode_completion(self):\n",
    "        # Implement your logic to check if the episode is completed\n",
    "      return\n",
    "    def write_frame_to_output_video(self, frame, output_video):\n",
    "        # Implement your logic to write frame with bounding boxes and labels to output video\n",
    "        return\n",
    "    def print_detected_objects_summary(self):\n",
    "        # Implement your logic to print the summary of detected objects\n",
    "        return\n",
    "    def print_tracked_objects(self):\n",
    "        # Implement your logic to print the tracked objects\n",
    "        return\n",
    "    # ... (other methods)\n",
    "    def get_current_state(self):\n",
    "        # Implement your logic to get the current state as a tensor\n",
    "        # Example: Convert the current frame to a tensor\n",
    "        # Note: You may need to preprocess the frame\n",
    "        frame = self.get_current_frame()  # Implement this function\n",
    "        state = torch.tensor(frame, dtype=torch.float32)\n",
    "        return state\n",
    "\n",
    "    def get_current_frame(self):\n",
    "        # Implement your logic to get the current frame\n",
    "        # Example: Capture the current frame from video\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            return None\n",
    "        return frame\n",
    "\n",
    "    def get_frame_dimensions(self):\n",
    "        # Implement your logic to get the frame dimensions\n",
    "        frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        return frame_width, frame_height\n",
    "\n",
    "    def get_current_bounding_box(self):\n",
    "        # Assuming each object is represented as a dictionary with keys 'x', 'y', 'w', 'h'\n",
    "        # and the current tracked object is identified by self.current_object_id\n",
    "        if self.current_object_id in self.tracking_objects:\n",
    "            current_object = self.tracking_objects[self.current_object_id]\n",
    "            current_bounding_box = (current_object['x'], current_object['y'], current_object['w'], current_object['h'])\n",
    "            return current_bounding_box\n",
    "        else:\n",
    "            # If the current object is not found in the tracking_objects dictionary\n",
    "            # You may return a default value or handle the situation accordingly\n",
    "            return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create an instance of the ObjectDetection class\n",
    "    od = ObjectDetection(model_config='dnn_model/yolov4.cfg', model_weights='dnn_model/yolov4.weights',\n",
    "                         classes_path='dnn_model/classes.txt')\n",
    "\n",
    "    # Specify the paths\n",
    "    dqn_model_path = 'modell.h5'\n",
    "    video_path = 'output.mp4'\n",
    "    output_video_path = 'tracked_output.mp4'\n",
    "\n",
    "    # Create an instance of the ObjectTracker class and run the object tracking\n",
    "    tracker = ObjectTracker(od, dqn_model_path, video_path, output_video_path)\n",
    "    tracker.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Object Detection\n",
      "Running YOLOv8\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Define the ObjectDetection class for YOLOv4\n",
    "class ObjectDetection:\n",
    "    def __init__(self, model_config='dnn_model/yolov4.cfg', model_weights='dnn_model/yolov4.weights', classes_path='dnn_model/classes.txt'):\n",
    "        print(\"Loading Object Detection\")\n",
    "        print(\"Running YOLOv8\")\n",
    "\n",
    "        self.confThreshold = 0.5\n",
    "        self.nmsThreshold = 0.4\n",
    "\n",
    "        # Load the YOLOv4 model\n",
    "        self.net = cv2.dnn.readNet(model_config, model_weights)\n",
    "        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "        # Load the class names\n",
    "        self.load_class_names(classes_path)\n",
    "\n",
    "    def load_class_names(self, classes_path):\n",
    "        with open(classes_path, \"r\") as file_object:\n",
    "            self.classes = [class_name.strip() for class_name in file_object]\n",
    "\n",
    "        self.colors = np.random.uniform(0, 255, size=(len(self.classes), 3))\n",
    "\n",
    "    def detect(self, frame, target_object):\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "        output_layers = self.net.getUnconnectedOutLayersNames()\n",
    "        outputs = self.net.forward(output_layers)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        for output in outputs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > self.confThreshold:\n",
    "                    class_name = self.classes[class_id]\n",
    "\n",
    "                    if class_name == target_object:\n",
    "                        center_x = int(detection[0] * width)\n",
    "                        center_y = int(detection[1] * height)\n",
    "                        w = int(detection[2] * width)\n",
    "                        h = int(detection[3] * height)\n",
    "                        x = int(center_x - w / 2)\n",
    "                        y = int(center_y - h / 2)\n",
    "\n",
    "                        class_ids.append(int(class_id))\n",
    "                        confidences.append(float(confidence))\n",
    "                        boxes.append((x, y, w, h))\n",
    "\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, self.confThreshold, self.nmsThreshold)\n",
    "        filtered_indices = indices.flatten()\n",
    "        filtered_class_ids = [class_ids[i] for i in filtered_indices]\n",
    "        filtered_confidences = [confidences[i] for i in filtered_indices]\n",
    "        filtered_boxes = [boxes[i] for i in filtered_indices]\n",
    "\n",
    "        return filtered_class_ids, filtered_confidences, filtered_boxes\n",
    "\n",
    "# Define the ObjectTracker class\n",
    "class ObjectTracker:\n",
    "    def __init__(self, object_detection, video_path, output_video_path, target_object):\n",
    "        self.object_detection = object_detection\n",
    "        self.video_path = video_path\n",
    "        self.output_video_path = output_video_path\n",
    "        self.target_object = target_object\n",
    "\n",
    "        self.tracking_objects = {}\n",
    "\n",
    "        # Initialize the video capture (cap)\n",
    "        self.cap = cv2.VideoCapture(self.video_path)\n",
    "\n",
    "    def run(self):\n",
    "        # Initialize the output video writer\n",
    "        output_video = self.initialize_output_video_writer(self.cap)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Perform object detection for the target object\n",
    "            detections = self.object_detection.detect(frame, self.target_object)\n",
    "\n",
    "            # Update tracking objects\n",
    "            self.update_tracking_objects(detections)\n",
    "\n",
    "            # Draw bounding boxes and labels\n",
    "            self.draw_boxes(frame)\n",
    "\n",
    "            # Write frame with bounding boxes and labels to output video\n",
    "            self.write_frame_to_output_video(frame, output_video)\n",
    "\n",
    "            # Show the frame with the detected and tracked objects\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        # Release video capture and close windows\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def initialize_output_video_writer(self, cap):\n",
    "        output_video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        output_video = cv2.VideoWriter(self.output_video_path, fourcc, output_video_fps, (frame_width, frame_height), True)\n",
    "        return output_video\n",
    "\n",
    "    def update_tracking_objects(self, detections):\n",
    "        class_ids, confidences, boxes = detections\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            class_id = class_ids[i]\n",
    "            confidence = confidences[i]\n",
    "            (x, y, w, h) = box\n",
    "\n",
    "            # Create or update the tracking object using class_id as the identifier\n",
    "            self.tracking_objects[class_id] = {\n",
    "                'class_id': class_id,\n",
    "                'confidence': confidence,\n",
    "                'box': (x, y, w, h)\n",
    "            }\n",
    "\n",
    "    def draw_boxes(self, frame):\n",
    "        for obj_id, obj_info in self.tracking_objects.items():\n",
    "            class_id = obj_info['class_id']\n",
    "            confidence = obj_info['confidence']\n",
    "            (x, y, w, h) = obj_info['box']\n",
    "\n",
    "            color = self.object_detection.colors[class_id]\n",
    "            label = f\"{self.object_detection.classes[class_id]}: {confidence:.2f}\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    def write_frame_to_output_video(self, frame, output_video):\n",
    "        output_video.write(frame)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create an instance of the ObjectDetection class\n",
    "    od = ObjectDetection(model_config='dnn_model/yolov4.cfg', model_weights='dnn_model/yolov4.weights', classes_path='dnn_model/classes.txt')\n",
    "\n",
    "    # Specify the paths and target object\n",
    "    video_path = 'output.mp4'\n",
    "    output_video_path = 'tracked_output.mp4'\n",
    "    target_object = 'car'  # Specify the object you want to prioritize (e.g., 'car')\n",
    "\n",
    "    # Create an instance of the ObjectTracker class and run the object tracking\n",
    "    tracker = ObjectTracker(od, video_path, output_video_path, target_object)\n",
    "    tracker.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Object Detection\n",
      "Running YOLOv8\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m# Rest of the ObjectTracker and main code remains the same\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Create an instance of the ObjectDetection class\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     od \u001b[39m=\u001b[39m ObjectDetection(model_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39myolov8x.pt\u001b[39;49m\u001b[39m'\u001b[39;49m, classes_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdnn_model/classes.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     58\u001b[0m     \u001b[39m# Specify the paths and target object\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     video_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39moutput.mp4\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[25], line 20\u001b[0m, in \u001b[0;36mObjectDetection.__init__\u001b[0;34m(self, model_path, classes_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Load the YOLOv4 model using PyTorch\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(model_path)  \u001b[39m# Load your pretrained YOLOv4 model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet\u001b[39m.\u001b[39;49meval()\n\u001b[1;32m     22\u001b[0m \u001b[39m# Load the class names\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_class_names(classes_path)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Define the ObjectDetection class for YOLOv4 using PyTorch\n",
    "class ObjectDetection:\n",
    "    def __init__(self, model_path='yolov8x.pt', classes_path='dnn_model/classes.txt'):\n",
    "        print(\"Loading Object Detection\")\n",
    "        print(\"Running YOLOv8\")\n",
    "\n",
    "        self.confThreshold = 0.5\n",
    "        self.nmsThreshold = 0.4\n",
    "\n",
    "        # Load the YOLOv4 model using PyTorch\n",
    "        self.net = torch.load(model_path)  # Load your pretrained YOLOv4 model\n",
    "        self.net.eval()\n",
    "\n",
    "        # Load the class names\n",
    "        self.load_class_names(classes_path)\n",
    "\n",
    "    def load_class_names(self, classes_path):\n",
    "        with open(classes_path, \"r\") as file_object:\n",
    "            self.classes = [class_name.strip() for class_name in file_object]\n",
    "\n",
    "        self.colors = np.random.uniform(0, 255, size=(len(self.classes), 3))\n",
    "\n",
    "    def detect(self, frame, target_object):\n",
    "        # Perform object detection using the YOLOv4 model\n",
    "        results = self.net(frame)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "\n",
    "        for result in results.pred[0]:\n",
    "            class_id = int(result[-1])\n",
    "            class_name = self.classes[class_id]\n",
    "            confidence = float(result[4])\n",
    "\n",
    "            if confidence > self.confThreshold and class_name == target_object:\n",
    "                xyxy = result[:4].tolist()\n",
    "                boxes.append([int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])])\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(confidence)\n",
    "\n",
    "        return class_ids, confidences, boxes\n",
    "\n",
    "# Rest of the ObjectTracker and main code remains the same\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create an instance of the ObjectDetection class\n",
    "    od = ObjectDetection(model_path='yolov8x.pt', classes_path='dnn_model/classes.txt')\n",
    "\n",
    "    # Specify the paths and target object\n",
    "    video_path = 'output.mp4'\n",
    "    output_video_path = 'tracked_output.mp4'\n",
    "    target_object = 'car'  # Specify the object you want to prioritize (e.g., 'car')\n",
    "\n",
    "    # Create an instance of the ObjectTracker class and run the object tracking\n",
    "    tracker = ObjectTracker(od, video_path, output_video_path, target_object)\n",
    "    tracker.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 🚀 Python-3.11.1 torch-2.0.1 CPU\n",
      "Setup complete ✅ (8 CPUs, 8.0 GB RAM, 182.2/228.3 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "%pip install ultralytics\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class_ids = [2, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv8_ObjectDetector:\n",
    "    \"\"\"\n",
    "    A class for performing object detection on images and videos using YOLOv8.\n",
    "\n",
    "    Args:\n",
    "    ------------\n",
    "        model_file (str): Path to the YOLOv8 model file or yolo model variant name in ths format: [variant].pt\n",
    "        labels (list[str], optional): A list of class labels for the model. If None, uses the default labels from the model file.\n",
    "        classes (list[str], optional): Alias for labels. Deprecated.\n",
    "        conf (float, optional): Minimum confidence threshold for object detection.\n",
    "        iou (float, optional): Minimum IOU threshold for non-max suppression.\n",
    "\n",
    "    Attributes:\n",
    "    --------------\n",
    "        classes (list[str]): A list of class labels for the model ( a Dict is also acceptable).\n",
    "        conf (float): Minimum confidence threshold for object detection.\n",
    "        iou (float): Minimum IOU threshold for non-max suppression.\n",
    "        model (YOLO): The YOLOv8 model used for object detection.\n",
    "        model_name (str): The name of the YOLOv8 model file (without the .pt extension).\n",
    "\n",
    "    Methods :\n",
    "    -------------\n",
    "        default_display: Returns a default display (ultralytics plot implementation) of the object detection results.\n",
    "        custom_display: Returns a custom display of the object detection results.\n",
    "        predict_video: Predicts objects in a video and saves the results to a file.\n",
    "        predict_img: Predicts objects in an image and returns the detection results.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_file = 'yolov8x.pt', labels= None, classes = None, conf = 0.25, iou = 0.45 ):\n",
    "\n",
    "        self.classes = classes\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "\n",
    "        self.model = YOLO(model_file)\n",
    "        self.model_name = model_file.split('.')[0]\n",
    "        self.results = None\n",
    "\n",
    "        if labels == None:\n",
    "            self.labels = self.model.names\n",
    "\n",
    "    def predict_img(self, img, verbose=True):\n",
    "        \"\"\"\n",
    "        Runs object detection on a single image and filter objects based on target class IDs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img (numpy.ndarray): Input image to perform object detection on.\n",
    "        verbose (bool): Whether to print detection details.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        'ultralytics.yolo.engine.results.Results': A YOLO results object that contains \n",
    "         details about detection results:\n",
    "            - Class IDs\n",
    "            - Bounding Boxes\n",
    "            - Confidence score\n",
    "            ...\n",
    "        (please refer to https://docs.ultralytics.com/reference/results/#results-api-reference for results API reference)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the model on the input image with the given parameters\n",
    "        results = self.model(img, classes=self.classes, conf=self.conf, iou=self.iou, verbose=verbose)\n",
    "\n",
    "        # Save the original image and the results for further analysis if needed\n",
    "        self.orig_img = img\n",
    "\n",
    "        # Filter objects based on target class IDs\n",
    "        filtered_results = self.filter_objects_by_class_ids(results, target_class_ids)\n",
    "\n",
    "        # Return the filtered detection results\n",
    "        return filtered_results\n",
    "\n",
    "    def filter_objects_by_class_ids(self, results, target_class_ids):\n",
    "        \"\"\"\n",
    "        Filter detection results based on target class IDs.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        results: YOLO detection results.\n",
    "        target_class_ids (list): A list of target class IDs to keep.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        'ultralytics.yolo.engine.results.Results': A YOLO results object containing filtered detection results.\n",
    "        \"\"\"\n",
    "        filtered_boxes = []\n",
    "\n",
    "        for box in results.pred[0]:\n",
    "            class_id = int(box[5])\n",
    "\n",
    "            if class_id in target_class_ids:\n",
    "                filtered_boxes.append(box)\n",
    "\n",
    "        filtered_results = results.clone()\n",
    "        filtered_results.pred[0] = np.stack(filtered_boxes)\n",
    "\n",
    "        return filtered_results\n",
    "\n",
    "\n",
    "    def track_specific_objects(self, results, target_class_ids):\n",
    "        \"\"\"\n",
    "        Track specific objects based on their class IDs.\n",
    "\n",
    "        Args:\n",
    "            results: YOLO detection results.\n",
    "            target_class_ids (list): A list of target class IDs to track.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tracked objects with the specified class IDs.\n",
    "        \"\"\"\n",
    "        tracked_objects = []\n",
    "\n",
    "        for box in results.pred[0]:\n",
    "            class_id = int(box[5])\n",
    "\n",
    "            if class_id in target_class_ids:\n",
    "                # Extract object details for tracking (e.g., bounding box coordinates)\n",
    "                x1, y1, x2, y2 = map(int, box[:4])\n",
    "                tracked_objects.append((class_id, (x1, y1, x2, y2)))\n",
    "\n",
    "        return tracked_objects\n",
    "    def default_display(self, show_conf=True, line_width=None, font_size=None, \n",
    "                        font='Arial.ttf', pil=False, example='abc'):\n",
    "        \"\"\"\n",
    "        Displays the detected objects on the original input image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        show_conf : bool, optional\n",
    "            Whether to show the confidence score of each detected object, by default True.\n",
    "        line_width : int, optional\n",
    "            The thickness of the bounding box line in pixels, by default None.\n",
    "        font_size : int, optional\n",
    "            The font size of the text label for each detected object, by default None.\n",
    "        font : str, optional\n",
    "            The font type of the text label for each detected object, by default 'Arial.ttf'.\n",
    "        pil : bool, optional\n",
    "            Whether to return a PIL Image object, by default False.\n",
    "        example : str, optional\n",
    "            A string to display on the example bounding box, by default 'abc'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray or PIL Image\n",
    "            The original input image with the detected objects displayed as bounding boxes.\n",
    "            If `pil=True`, a PIL Image object is returned instead.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the input image has not been detected by calling the `predict_img()` method first.\n",
    "        \"\"\"\n",
    "        # Check if the `predict_img()` method has been called before displaying the detected objects\n",
    "        if self.results is None:\n",
    "            raise ValueError('No detected objects to display. Call predict_img() method first.')\n",
    "        \n",
    "        # Call the plot() method of the `self.results` object to display the detected objects on the original image\n",
    "        display_img = self.results.plot(show_conf, line_width, font_size, font, pil, example)\n",
    "        \n",
    "        # Return the displayed image\n",
    "        return display_img\n",
    "\n",
    "        \n",
    "\n",
    "    def custom_display(self, colors, show_cls = True, show_conf = True):\n",
    "        \"\"\"\n",
    "        Custom display method that draws bounding boxes and labels on the original image, \n",
    "        with additional options for showing class and confidence information.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        colors : list\n",
    "            A list of tuples specifying the color of each class.\n",
    "        show_cls : bool, optional\n",
    "            Whether to show class information in the label text. Default is True.\n",
    "        show_conf : bool, optional\n",
    "            Whether to show confidence information in the label text. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            The image with bounding boxes and labels drawn on it.\n",
    "        \"\"\"\n",
    "\n",
    "        img = self.orig_img\n",
    "        # calculate the bounding box thickness based on the image width and height\n",
    "        bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
    "\n",
    "        for box in self.results.boxes:\n",
    "            textString = \"\"\n",
    "\n",
    "            # Extract object class and confidence score\n",
    "            score = box.conf.item() * 100\n",
    "            class_id = int(box.cls.item())\n",
    "\n",
    "            x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
    "\n",
    "            # Print detection info\n",
    "            if show_cls:\n",
    "                textString += f\"{self.labels[class_id]}\"\n",
    "\n",
    "            if show_conf:\n",
    "                textString += f\" {score:,.2f}%\"\n",
    "\n",
    "            # Calculate font scale based on object size\n",
    "            font = cv2.FONT_HERSHEY_COMPLEX\n",
    "            fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
    "            fontThickness = 1\n",
    "            textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
    "\n",
    "            # Draw bounding box, a centroid and label on the image\n",
    "            img = cv2.rectangle(img, (x1,y1), (x2,y2), colors[class_id], bbx_thickness)\n",
    "            center_coordinates = ((x1 + x2)//2, (y1 + y2) // 2)\n",
    "\n",
    "            img =  cv2.circle(img, center_coordinates, 5 , (0,0,255), -1)\n",
    "            \n",
    "             # If there are no details to show on the image\n",
    "            if textString != \"\":\n",
    "                if (y1 < textSize[1]):\n",
    "                    y1 = y1 + textSize[1]\n",
    "                else:\n",
    "                    y1 -= 2\n",
    "                # show the details text in a filled rectangle\n",
    "                img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0] , y1 -  textSize[1]), colors[class_id], cv2.FILLED)\n",
    "                img = cv2.putText(img, textString , \n",
    "                    (x1, y1), font, \n",
    "                    fontScale,  (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def predict_video(self, video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args):\n",
    "        \"\"\"Runs object detection on each frame of a video and saves the output to a new video file.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "            video_path (str): The path to the input video file.\n",
    "            save_dir (str): The path to the directory where the output video file will be saved.\n",
    "            save_format (str, optional): The format for the output video file. Defaults to \"avi\".\n",
    "            display (str, optional): The type of display for the detection results. Defaults to 'custom'.\n",
    "            verbose (bool, optional): Whether to print information about the video file and output file. Defaults to True.\n",
    "            **display_args: Additional arguments to be passed to the display function.\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Open the input video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Get the name of the input video file\n",
    "        vid_name = os.path.basename(video_path)\n",
    "\n",
    "        # Get the dimensions of each frame in the input video file\n",
    "        width = int(cap.get(3))  # get `width`\n",
    "        height = int(cap.get(4))  # get `height`\n",
    "\n",
    "        # Create the directory for the output video file if it does not already exist\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # Set the name and path for the output video file\n",
    "        save_name = self.model_name + ' -- ' + vid_name.split('.')[0] + '.' + save_format\n",
    "        save_file = os.path.join(save_dir, save_name)\n",
    "\n",
    "        # Print information about the input and output video files if verbose is True\n",
    "        if verbose:\n",
    "            print(\"----------------------------\")\n",
    "            print(f\"DETECTING OBJECTS IN : {vid_name} : \")\n",
    "            print(f\"RESOLUTION : {width}x{height}\")\n",
    "            print('SAVING TO :' + save_file)\n",
    "\n",
    "        # Define an output VideoWriter object\n",
    "        out = cv2.VideoWriter(save_file,\n",
    "                              cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
    "                              30, (width, height))\n",
    "\n",
    "        # Check if the input video file was opened correctly\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error opening video stream or file\")\n",
    "\n",
    "        # Read each frame of the input video file\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # If the frame was not read successfully, break the loop\n",
    "            if not ret:\n",
    "                print(\"Error reading frame\")\n",
    "                break\n",
    "\n",
    "            # Run object detection on the frame and calculate FPS\n",
    "            beg = time.time()\n",
    "            results = self.predict_img(frame, verbose=False)\n",
    "            if results is None:\n",
    "                print('***********************************************')\n",
    "            fps = 1 / (time.time() - beg)\n",
    "\n",
    "            # Display the detection results\n",
    "            if display == 'default':\n",
    "                frame = self.default_display(**display_args)\n",
    "            elif display == 'custom':\n",
    "                frame == self.custom_display(**display_args)\n",
    "\n",
    "            # Display the FPS on the frame\n",
    "            frame = cv2.putText(frame, f\"FPS : {fps:,.2f}\",\n",
    "                                (5, 15), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Write the frame to the output video file\n",
    "            out.write(frame)\n",
    "\n",
    "            # Exit the loop if the 'q' button is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # After the loop release the cap and video writer\n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "d = YOLOv8_ObjectDetector()\n",
    "print(d.labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize YOLOv8 detectors with different model files and confidence thresholds\n",
    "yolo_names = ['yolov8x.pt']\n",
    "colors = [(random.randint(50, 255), random.randint(50, 255), random.randint(50, 255)) for _ in range(80)]\n",
    "detectors = []\n",
    "\n",
    "for yolo_name in yolo_names:\n",
    "    detector = YOLOv8_ObjectDetector(yolo_name, conf=0.55)\n",
    "    detectors.append(detector)\n",
    "\n",
    "# Process a test video and track specific objects\n",
    "vid_results_path = 'ultralytics/ultralytics/tracker/utils/output'\n",
    "test_vids_path = 'ultralytics/ultralytics/tracker/trackers/test vids'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "DETECTING OBJECTS IN : output.mp4 : \n",
      "RESOLUTION : 1024x540\n",
      "SAVING TO :ultralytics/ultralytics/tracker/utils/output/yolov8x -- output.avi\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(vid_results_path)\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m detector \u001b[39min\u001b[39;00m detectors:\n\u001b[0;32m---> 17\u001b[0m     detector\u001b[39m.\u001b[39;49mpredict_video(\n\u001b[1;32m     18\u001b[0m         video_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39multralytics/ultralytics/tracker/trackers/test vids/output.mp4\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m         save_dir\u001b[39m=\u001b[39;49mvid_results_path,\n\u001b[1;32m     20\u001b[0m         save_format\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mavi\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m         display\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcustom\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m         colors\u001b[39m=\u001b[39;49mcolors,\n\u001b[1;32m     23\u001b[0m         target_class_ids\u001b[39m=\u001b[39;49mtarget_class_ids\n\u001b[1;32m     24\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[60], line 296\u001b[0m, in \u001b[0;36mYOLOv8_ObjectDetector.predict_video\u001b[0;34m(self, video_path, save_dir, save_format, display, verbose, **display_args)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39m# Run object detection on the frame and calculate FPS\u001b[39;00m\n\u001b[1;32m    295\u001b[0m beg \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 296\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_img(frame, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    297\u001b[0m \u001b[39mif\u001b[39;00m results \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m***********************************************\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[60], line 71\u001b[0m, in \u001b[0;36mYOLOv8_ObjectDetector.predict_img\u001b[0;34m(self, img, verbose)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_img \u001b[39m=\u001b[39m img\n\u001b[1;32m     70\u001b[0m \u001b[39m# Filter objects based on target class IDs\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m filtered_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilter_objects_by_class_ids(results, target_class_ids)\n\u001b[1;32m     73\u001b[0m \u001b[39m# Return the filtered detection results\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m filtered_results\n",
      "Cell \u001b[0;32mIn[60], line 91\u001b[0m, in \u001b[0;36mYOLOv8_ObjectDetector.filter_objects_by_class_ids\u001b[0;34m(self, results, target_class_ids)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39mFilter detection results based on target class IDs.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39m'ultralytics.yolo.engine.results.Results': A YOLO results object containing filtered detection results.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m filtered_boxes \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 91\u001b[0m \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39;49mpred[\u001b[39m0\u001b[39m]:\n\u001b[1;32m     92\u001b[0m     class_id \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(box[\u001b[39m5\u001b[39m])\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m class_id \u001b[39min\u001b[39;00m target_class_ids:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'pred'"
     ]
    }
   ],
   "source": [
    "yolo_names = ['yolov8x.pt']\n",
    "colors = [(random.randint(50, 255), random.randint(50, 255), random.randint(50, 255)) for _ in range(80)]\n",
    "detectors = []\n",
    "\n",
    "for yolo_name in yolo_names:\n",
    "    detector = YOLOv8_ObjectDetector(yolo_name, conf=0.55)\n",
    "    detectors.append(detector)\n",
    "\n",
    "# Process a test video and track specific objects\n",
    "vid_results_path = 'ultralytics/ultralytics/tracker/utils/output'\n",
    "test_vids_path = 'ultralytics/ultralytics/tracker/trackers/test vids'\n",
    "\n",
    "if not os.path.isdir(vid_results_path):\n",
    "    os.makedirs(vid_results_path)\n",
    "\n",
    "for detector in detectors:\n",
    "    detector.predict_video(\n",
    "        video_path='ultralytics/ultralytics/tracker/trackers/test vids/output.mp4',\n",
    "        save_dir=vid_results_path,\n",
    "        save_format=\"avi\",\n",
    "        display='custom',\n",
    "        colors=colors,\n",
    "        target_class_ids=target_class_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "DETECTING OBJECTS IN : output.mp4 : \n",
      "RESOLUTION : 1024x540\n",
      "SAVING TO :ultralytics/ultralytics/tracker/utils/output/yolov8n -- output.avi\n",
      "Error reading frame\n",
      "----------------------------\n",
      "DETECTING OBJECTS IN : output.mp4 : \n",
      "RESOLUTION : 1024x540\n",
      "SAVING TO :ultralytics/ultralytics/tracker/utils/output/yolov8x -- output.avi\n",
      "Error reading frame\n"
     ]
    }
   ],
   "source": [
    "for detector in detectors:\n",
    "    detector.predict_video(video_path= 'ultralytics/ultralytics/tracker/trackers/test vids/output.mp4'\n",
    ", save_dir = vid_results_path, save_format = \"avi\", display = 'custom', colors = colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "DETECTING OBJECTS IN: output.mp4\n",
      "RESOLUTION: 1024x540\n",
      "SAVING TO: ultralytics/ultralytics/tracker/utils/output/yolov8x -- output.avi\n",
      "Error reading frame\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import ssl\n",
    "from ultralytics import YOLO\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Load YOLOv8 model\n",
    "yolo_names = ['yolov8x.pt']\n",
    "yolov8_models = {}\n",
    "for yolo_name in yolo_names:\n",
    "    yolov8_models[yolo_name[:-3]] = YOLO(yolo_name)\n",
    "\n",
    "class YOLOv8_ObjectDetector:\n",
    "    def __init__(self, model_file='yolov8x.pt', conf=0.25, iou=0.45):\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "        self.model = yolov8_models[model_file.split('.')[0]]\n",
    "        self.model_name = model_file.split('.')[0]\n",
    "        self.results = None\n",
    "\n",
    "    def predict_img(self, img, verbose=True):\n",
    "        results = self.model(img, conf=self.conf, iou=self.iou, verbose=verbose)\n",
    "        self.orig_img = img\n",
    "        self.results = results[0]\n",
    "        return results[0]\n",
    "\n",
    "    def custom_display(self, colors, show_cls=True, show_conf=True):\n",
    "        img = self.orig_img\n",
    "        bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
    "\n",
    "        for box in self.results.boxes:\n",
    "            textString = \"\"\n",
    "            score = box.conf.item() * 100\n",
    "            class_id = int(box.cls.item())\n",
    "            x1, y1, x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
    "\n",
    "            if show_cls:\n",
    "                textString += f\"{self.model.names[class_id]}\"\n",
    "\n",
    "            if show_conf:\n",
    "                textString += f\" {score:,.2f}%\"\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_COMPLEX\n",
    "            fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
    "            fontThickness = 1\n",
    "            textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
    "\n",
    "            img = cv2.rectangle(img, (x1, y1), (x2, y2), colors[class_id], bbx_thickness)\n",
    "            center_coordinates = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "            img = cv2.circle(img, center_coordinates, 5, (0, 0, 255), -1)\n",
    "\n",
    "            if textString != \"\":\n",
    "                if (y1 < textSize[1]):\n",
    "                    y1 = y1 + textSize[1]\n",
    "                else:\n",
    "                    y1 -= 2\n",
    "                img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0], y1 - textSize[1]), colors[class_id], cv2.FILLED)\n",
    "                img = cv2.putText(img, textString, (x1, y1), font, fontScale, (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def predict_video(self, video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        vid_name = os.path.basename(video_path)\n",
    "        width = int(cap.get(3))\n",
    "        height = int(cap.get(4))\n",
    "\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        save_name = self.model_name + ' -- ' + vid_name.split('.')[0] + '.' + save_format\n",
    "        save_file = os.path.join(save_dir, save_name)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"----------------------------\")\n",
    "            print(f\"DETECTING OBJECTS IN: {vid_name}\")\n",
    "            print(f\"RESOLUTION: {width}x{height}\")\n",
    "            print('SAVING TO: ' + save_file)\n",
    "\n",
    "        out = cv2.VideoWriter(save_file, cv2.VideoWriter_fourcc(*\"MJPG\"), 30, (width, height), True)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error opening video stream or file\")\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                print(\"Error reading frame\")\n",
    "                break\n",
    "\n",
    "            beg = time.time()\n",
    "            results = self.predict_img(frame, verbose=False)\n",
    "            if results is None:\n",
    "                print('***********************************************')\n",
    "            fps = 1 / (time.time() - beg)\n",
    "\n",
    "            if display == 'custom':\n",
    "                frame = self.custom_display(**display_args)\n",
    "\n",
    "            frame = cv2.putText(frame, f\"FPS: {fps:,.2f}\", (5, 15), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "# Initialize YOLOv8 detectors with different model files and confidence thresholds\n",
    "yolo_names = ['yolov8x.pt']\n",
    "colors = [(random.randint(50, 255), random.randint(50, 255), random.randint(50, 255)) for _ in range(80)]\n",
    "detectors = []\n",
    "\n",
    "for yolo_name in yolo_names:\n",
    "    detector = YOLOv8_ObjectDetector(yolo_name, conf=0.55)\n",
    "    detectors.append(detector)\n",
    "\n",
    "# Process a test video and track specific objects\n",
    "vid_results_path = 'ultralytics/ultralytics/tracker/utils/output'\n",
    "test_vids_path = 'ultralytics/ultralytics/tracker/trackers/test vids'\n",
    "\n",
    "if not os.path.isdir(vid_results_path):\n",
    "    os.makedirs(vid_results_path)\n",
    "\n",
    "for detector in detectors:\n",
    "    detector.predict_video(\n",
    "        video_path='ultralytics/ultralytics/tracker/trackers/test vids/output.mp4',\n",
    "        save_dir=vid_results_path,\n",
    "        save_format=\"avi\",\n",
    "        display='custom',\n",
    "        colors=colors\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading frame\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import ssl\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from collections import deque\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Load YOLOv8 model\n",
    "yolo_names = ['yolov8x.pt']\n",
    "yolov8_models = {}\n",
    "for yolo_name in yolo_names:\n",
    "    yolov8_models[yolo_name[:-3]] = YOLO(yolo_name)\n",
    "\n",
    "# Define colors for bounding boxes\n",
    "colors = [(random.randint(50, 255), random.randint(50, 255), random.randint(50, 255)) for _ in range(80)]\n",
    "\n",
    "# Initialize DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "state_size = 150528  # Define the state size based on your specific requirements\n",
    "action_size = 2  # Define the action size based on your specific requirements\n",
    "\n",
    "dqn_model_path = 'modell.h5'\n",
    "dqn_model = DQN(state_size, action_size)\n",
    "dqn_model.load_state_dict(torch.load(dqn_model_path))\n",
    "dqn_model.eval()\n",
    "\n",
    "# Initialize Kalman filters for tracking\n",
    "kalman_filters = {}  # Dictionary to store Kalman filters for each object\n",
    "\n",
    "# Initialize a deque to store the last N Kalman filter predictions\n",
    "N_HISTORY = 10  # Define the number of history states to store for each object\n",
    "kalman_history = {}  # Dictionary to store Kalman prediction history for each object\n",
    "\n",
    "# Initialize Hungarian algorithm cost matrix\n",
    "cost_matrix = None\n",
    "\n",
    "# Initialize prioritization weights for tracked objects\n",
    "prioritization_weights = {}  # Dictionary to store weights for each tracked object\n",
    "\n",
    "# Define YOLOv8 object detection class\n",
    "class YOLOv8_ObjectDetector:\n",
    "    def __init__(self, model_file='yolov8x.pt', conf=0.25, iou=0.45):\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "        self.model = yolov8_models[model_file.split('.')[0]]\n",
    "        self.model_name = model_file.split('.')[0]\n",
    "        self.results = None\n",
    "\n",
    "    def predict_img(self, img, verbose=True):\n",
    "        results = self.model(img, conf=self.conf, iou=self.iou, verbose=verbose)\n",
    "        self.orig_img = img\n",
    "        self.results = results[0]\n",
    "        return results[0]\n",
    "\n",
    "    def custom_display(self, show_cls=True, show_conf=True):\n",
    "        img = self.orig_img\n",
    "        bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
    "\n",
    "        for box in self.results.boxes:\n",
    "            textString = \"\"\n",
    "            score = box.conf.item() * 100\n",
    "            class_id = int(box.cls.item())\n",
    "            x1, y1, x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
    "\n",
    "            if show_cls:\n",
    "                textString += f\"{self.model.names[class_id]}\"\n",
    "\n",
    "            if show_conf:\n",
    "                textString += f\" {score:,.2f}%\"\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_COMPLEX\n",
    "            fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
    "            fontThickness = 1\n",
    "            textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
    "\n",
    "            img = cv2.rectangle(img, (x1, y1), (x2, y2), colors[class_id], bbx_thickness)\n",
    "            center_coordinates = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "            img = cv2.circle(img, center_coordinates, 5, (0, 0, 255), -1)\n",
    "\n",
    "            if textString != \"\":\n",
    "                if (y1 < textSize[1]):\n",
    "                    y1 = y1 + textSize[1]\n",
    "                else:\n",
    "                    y1 -= 2\n",
    "                img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0], y1 - textSize[1]), colors[class_id], cv2.FILLED)\n",
    "                img = cv2.putText(img, textString, (x1, y1), font, fontScale, (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "# Initialize YOLOv8 detectors with different model files and confidence thresholds\n",
    "yolo_names = ['yolov8x.pt']\n",
    "detectors = []\n",
    "\n",
    "for yolo_name in yolo_names:\n",
    "    detector = YOLOv8_ObjectDetector(yolo_name, conf=0.55)\n",
    "    detectors.append(detector)\n",
    "\n",
    "# Process a test video and track specific objects\n",
    "vid_results_path = 'ultralytics/ultralytics/tracker/utils/output'\n",
    "test_vids_path = 'ultralytics/ultralytics/tracker/trackers/test vids'\n",
    "\n",
    "if not os.path.isdir(vid_results_path):\n",
    "    os.makedirs(vid_results_path)\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture('ultralytics/ultralytics/tracker/trackers/test vids/output.mp4')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error reading frame\")\n",
    "        break\n",
    "\n",
    "    for detector in detectors:\n",
    "        results = detector.predict_img(frame, verbose=False)\n",
    "        if results is None:\n",
    "            print('***********************************************')\n",
    "\n",
    "    # Implement tracking, Kalman filtering, Hungarian algorithm, and prioritization logic here\n",
    "\n",
    "    # Display the frame with tracked objects and prioritization weights\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
